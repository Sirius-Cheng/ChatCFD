import streamlit as st
import PyPDF2
import io
import json
from PIL import Image
import base64
import re
import tiktoken
from datetime import datetime

import config, case_file_requirements, preprocess_OF_tutorial, set_config, main_run_chatcfd, qa_modules
import pathlib
import os
from openai_client_factory import create_chat_client


general_prompt = ''


def _extract_json_dict(text: str):
    """Best-effort JSON parser that tolerates leading/trailing text."""
    if not text:
        return None
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        return None
    try:
        return json.loads(text[start : end + 1])
    except json.JSONDecodeError:
        return None


class ChatBot:
    def __init__(self):
        self.client = create_chat_client("DEEPSEEK_R1")
        self.model_name = os.environ.get("DEEPSEEK_R1_MODEL_NAME")
        # self.system_prompt = """You are an intelligent assistant capable of:
        # 1. Maintaining politeness and professionalism
        # 2. Remembering the context of the conversation
        # 3. Processing and analyzing content from documents uploaded by users
        # 4. Answering user questions while keeping the conversation coherent
        #
        # Please always respond in a clear, accurate, and helpful manner."""
        self.system_prompt = """ä½ æ˜¯ä¸€ä½æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿï¼š
        1. ä¿æŒç¤¼è²Œä¸ä¸“ä¸š
        2. è®°ä½å¯¹è¯ä¸Šä¸‹æ–‡
        3. å¤„ç†å¹¶åˆ†æç”¨æˆ·ä¸Šä¼ æ–‡æ¡£çš„å†…å®¹
        4. åœ¨ä¿æŒå¯¹è¯è¿è´¯çš„å‰æä¸‹å›ç­”ç”¨æˆ·é—®é¢˜

        è¯·å§‹ç»ˆä»¥æ¸…æ™°ã€å‡†ç¡®ä¸”æœ‰å¸®åŠ©çš„æ–¹å¼ä½œç­”ã€‚"""
        self.temperature = 0.9

        self.token_counter = {
            "total": 0,
            "qa_history": []
        }

    def process_pdf(self, pdf_file):
        try:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text
        except Exception as e:
            # return f"PDF processing error: {str(e)}"
            return f"PDF å¤„ç†å‡ºé”™ï¼š{str(e)}"

    def get_response(self, messages):

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "system", "content": self.system_prompt}] + messages,
                temperature=self.temperature,
                extra_body={"chat_template_kwargs": {"enable_thinking": False}},
            )
            # Record token usage
            usage = response.usage
            self.token_counter["total"] += usage.total_tokens
            qa_record = {
                "prompt": messages,
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens,
                "total_tokens": usage.total_tokens,
                "timestamp": datetime.now().isoformat()
            }
            return response.choices[0].message.content
        except Exception as e:
            # return f"Chat error: {str(e)}"
            return f"èŠå¤©å‡ºé”™ï¼š{str(e)}"

    def count_tokens(self, text: str, model: str = "gpt-4o") -> int:
        """Use tiktoken to count the number of tokens"""
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")
            return len(encoding.encode(text))

def initialize_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chatbot" not in st.session_state:
        st.session_state.chatbot = ChatBot()
    if "file_content" not in st.session_state:
        st.session_state.file_content = None
    if "file_processed" not in st.session_state:
        st.session_state.file_processed = False
    if "ask_case_solver" not in st.session_state:
        st.session_state.ask_case_solver = False
    if "user_answered" not in st.session_state:
        st.session_state.user_answered = False
    if "user_answer_finished" not in st.session_state:
        st.session_state.user_answer_finished = False
    if "uploaded_grid" not in st.session_state:
        st.session_state.uploaded_grid = False
    if "show_start" not in st.session_state:
        st.session_state.show_start = False

def extract_pure_response(text):
    # Use regex to match all content (including newlines)
    pattern = r"Here is my response:(.*?)(?=$|\Z)"
    match = re.search(pattern, text, re.DOTALL)
    
    if match:
        # Remove leading and trailing whitespace
        return match.group(1).strip()
    return ""

def test_function_call_by_QA():
    """Test function call"""
    # print("the test_function_call_by_QA() is called")  # Console print
    print("test_function_call_by_QA() æµ‹è¯•å‡½æ•°å·²è¢«è°ƒç”¨")  # Console print
    # return "âœ… Test function successfully called! System status normal."
    return "âœ… æµ‹è¯•å‡½æ•°è°ƒç”¨æˆåŠŸï¼ç³»ç»ŸçŠ¶æ€æ­£å¸¸ã€‚"
    

def main():

    # test other functions

    # test_function_call_by_QA()

    # a = 1

    # streamlit functions

    # st.title("ChatCFD: chat to run CFD cases.")
    st.title("ChatCFDï¼šé€šè¿‡èŠå¤©è¿è¡ŒCFDæ¡ˆä¾‹ã€‚")

    st.divider()
    
    initialize_session_state()

    with st.sidebar:

        # Export chat history functionality
        # st.header("Export chat history")
        st.header("å¯¼å‡ºèŠå¤©è®°å½•")
        export_format = "JSON"
        
        # if st.button("Export chat"):
        if st.button("å¯¼å‡ºå¯¹è¯"):
            if not st.session_state.messages:
                # st.warning("Empty chat history")
                st.warning("èŠå¤©è®°å½•ä¸ºç©º")
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"chatlog_{timestamp}"

                chat_data = {
                    "metadata": {
                        "export_time": datetime.now().isoformat(),
                        "total_messages": len(st.session_state.messages),
                        "total_tokens": st.session_state.chatbot.token_counter["total"]
                    },
                    "messages": st.session_state.messages
                }
                
                # st.sidebar.download_button(
                #     label="Download JSON file",
                #     data=json.dumps(chat_data, indent=2, ensure_ascii=False),
                #     file_name=f"{filename}.json",
                #     mime="application/json"
                # )
                st.sidebar.download_button(
                    label="ä¸‹è½½ JSON æ–‡ä»¶",
                    data=json.dumps(chat_data, indent=2, ensure_ascii=False),
                    file_name=f"{filename}.json",
                    mime="application/json"
                )

    # Sidebar: File Upload
    with st.sidebar:
        # st.header("Upload the document")
        st.header("ä¸Šä¼ æ–‡æ¡£")
        # uploaded_file = st.file_uploader(
        #     "Please upload PDF",
        #     type=['pdf']
        # )
        uploaded_file = st.file_uploader(
            "è¯·ä¸Šä¼  PDF",
            type=['pdf']
        )
        
        if uploaded_file:
            if not st.session_state.file_processed:
                if uploaded_file.type == "application/pdf":

                    save_dir = pathlib.Path(config.TEMP_PATH)
                    
                    try:
                        # Build save path
                        file_path = save_dir / uploaded_file.name.replace(" ", "_")
                        
                        # Save uploaded file
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())

                        config.pdf_path =  f"{config.TEMP_PATH}/{uploaded_file.name}"

                    except Exception as e:
                        # st.error(f"Failed at processed the pdf file: {str(e)}") 
                        st.error(f"å¤„ç† PDF æ–‡ä»¶å¤±è´¥ï¼š{str(e)}") 

                    text_content = st.session_state.chatbot.process_pdf(uploaded_file)
                    config.paper_content = text_content
                    # st.session_state.file_content = f"The  contentsï¼š\n{text_content}"
                    st.session_state.file_content = f"æ–‡æ¡£å†…å®¹ï¼š\n{text_content}"
                    # st.toast("PDF uploadedï¼", icon="ğŸ’¾")
                    st.toast("PDF å·²ä¸Šä¼ ï¼", icon="ğŸ’¾")
                    
                    # Add 1st question
                    question_1 = f'''æ‰€é™„çš„PDFåŒ…å«äº†å‡ ä¸ªCFDæ¡ˆä¾‹ï¼Œæˆ‘å¸Œæœ›ä¹‹åèƒ½äº²è‡ªè¿è¡Œå…¶ä¸­ä¸€ä¸ªæˆ–å‡ ä¸ªæ¡ˆä¾‹ã€‚è¯·é˜…è¯»è¿™ç¯‡è®ºæ–‡ï¼Œå¹¶åˆ—å‡ºæ‰€æœ‰ä¸åŒçš„CFDæ¡ˆä¾‹åŠå…¶ç‰¹å¾æè¿°ã€‚ä¸ºæ¯ä¸ªæ¡ˆä¾‹åˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼Œæ ¼å¼ä¸º Case_Xï¼ˆä¾‹å¦‚ Case_1, Case_2ï¼‰ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”

                    - è¯·å°†æ¯ä¸ªå¯¼è‡´ç‹¬ç«‹æ¨¡æ‹Ÿè¿è¡Œçš„å”¯ä¸€å‚æ•°ç»„åˆè®¡ä¸ºä¸€ä¸ªCFDæ¡ˆä¾‹ã€‚è¿™äº›å‚æ•°åŒ…æ‹¬ä½†ä¸é™äºï¼šå‡ ä½•å½¢çŠ¶ã€è¾¹ç•Œæ¡ä»¶ã€æµåŠ¨å‚æ•°ï¼ˆé›·è¯ºæ•°Re/é©¬èµ«æ•°Mach/æ”»è§’AoA/é€Ÿåº¦ï¼‰ã€ç‰©ç†æ¨¡å‹æˆ–æ±‚è§£å™¨ã€‚
                    - å¦‚æœä¸ºäº†ç»Ÿè®¡åˆ†ææˆ–æ”¶æ•›æ€§ç ”ç©¶è€Œå¯¹åŒä¸€ç»„å‚æ•°è¿›è¡Œäº†å¤šæ¬¡è¿è¡Œï¼Œè¯·å°†è¿™äº›è®¡ä¸ºä¸€ä¸ªæ¡ˆä¾‹ï¼Œé™¤éè®ºæ–‡å› å…¶ä¸åŒç›®æ ‡æˆ–æ¡ä»¶è€Œå°†å…¶æ˜ç¡®åŒºåˆ†ä¸ºä¸åŒæ¡ˆä¾‹ã€‚
                    - å¦‚æœæœ‰ä»»ä½•æ¡ˆä¾‹æ˜¯ä½¿ç”¨OpenFOAMè¿›è¡Œæ¨¡æ‹Ÿçš„ï¼Œè¯·è¯†åˆ«å…¶æ‰€ç”¨çš„æ±‚è§£å™¨ï¼Œæˆ–ä¸ºå…¶æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„æ±‚è§£å™¨ã€‚åœ¨æè¿°æ¡ˆä¾‹æ—¶ï¼Œè¯·æ³¨æ˜æ±‚è§£å™¨åç§°ã€‚
                    
                    è®ºæ–‡å†…å®¹å¦‚ä¸‹ï¼š \n{text_content}. 
                    '''

                    # question_1 = f'''The attached PDF contain several CFD cases, and I would like to run one or several of the case by my self later. Please read the paper and list all distinct CFD cases with characteristic description. Give each case a tag as Case_X (such as Case_1, Case_2).

                    # - Please count each unique combination of parameters that results in a separate simulation run as one CFD case. These parameters include but not limited to the geometry, boundary Conditions, flow Parameters (Re/Mach/AoA/velocity), physical Model, or Solver.
                    # - If there are multiple runs of the same parameters for statistical analysis or convergence studies, count these as one case, unless the paper specifies them as distinct due to different goals or conditions.
                    # - If any case is simulated using OpenFOAM, identify the solver or find a proper solver to run the case. Show the solver name when describing the case.
                    
                    # The paper is as follows: \n{text_content}. 
                    # '''
                    st.session_state.messages.append({
                        "role": "user",
                        "content": question_1, "timestamp": datetime.now().isoformat()
                    })
                    
                    # Get response for question A
                    with st.chat_message("assistant"):
                        response_1 = st.session_state.chatbot.get_response(st.session_state.messages)
                        st.write(response_1)
                        st.session_state.messages.append({"role": "assistant", "content": response_1, "timestamp": datetime.now().isoformat()})

                    st.session_state.file_processed = True

                    # Chatbot ask the user to choose case and solver
                    if not st.session_state.ask_case_solver:
                        # ask_to_choose_case_and_solver = '''Please choose the case you want to simulate and the OpenFOAM solver you want to use. 
                        #     Your answer shall be like one of the followings:\n- I want to simulate the Case with AOA = 10 degree and SpalartAllmaras model.\n- I want to simulate Case_1 using rhoCentralFoam and the SpalartAllmaras model.\n- I want to simulate the Case with AOA = 10 degree and kOmegaSST model.\n
                        #     
                        # \n You must choose only one case.
                        # '''
                        ask_to_choose_case_and_solver = '''è¯·é€‰æ‹©ä½ è¦æ¨¡æ‹Ÿçš„æ¡ˆä¾‹ä»¥åŠå¸Œæœ›ä½¿ç”¨çš„ OpenFOAM æ±‚è§£å™¨ã€‚
                            ä½ çš„å›ç­”å¯ä»¥å¦‚ä¸‹ï¼š\n- æˆ‘æƒ³æ¨¡æ‹Ÿæ”»è§’ AOA = 10Â° ä¸”é‡‡ç”¨ SpalartAllmaras æ¨¡å‹çš„æ¡ˆä¾‹ã€‚\n- æˆ‘æƒ³æ¨¡æ‹Ÿ Case_1ï¼Œå¹¶ä½¿ç”¨ rhoCentralFoam æ±‚è§£å™¨ä¸ SpalartAllmaras æ¨¡å‹ã€‚\n- æˆ‘æƒ³æ¨¡æ‹Ÿæ”»è§’ AOA = 10Â° ä¸”é‡‡ç”¨ kOmegaSST æ¨¡å‹çš„æ¡ˆä¾‹ã€‚\n
                            
                        \n ä½ å¿…é¡»ä¸”åªèƒ½é€‰æ‹©ä¸€ä¸ªæ¡ˆä¾‹ã€‚
                        '''
                        st.session_state.messages.append({
                            "role": "assistant", 
                            "content": ask_to_choose_case_and_solver,
                            "timestamp": datetime.now().isoformat()
                        })

                        st.session_state.ask_case_solver = True

    with st.sidebar:
        # st.header("Upload the mesh file")
        st.header("ä¸Šä¼ ç½‘æ ¼æ–‡ä»¶")
        # uploaded_mesh_file = st.file_uploader(
        #     "Please upload mesh (only support the Fluent-format .msh)",
        #     type=['msh']
        # )
        uploaded_mesh_file = st.file_uploader(
            "è¯·ä¸Šä¼ ç½‘æ ¼æ–‡ä»¶ï¼ˆä»…æ”¯æŒ Fluent æ ¼å¼ .mshï¼‰",
            type=['msh']
        )
        if uploaded_mesh_file:
            if not st.session_state.uploaded_grid:
                # Create save directory
                save_dir = pathlib.Path(config.TEMP_PATH)
                
                try:
                    # Build save path
                    file_path = save_dir / uploaded_mesh_file.name.replace(" ", "_")
                    
                    # Save uploaded file
                    with open(file_path, "wb") as f:
                        f.write(uploaded_mesh_file.getbuffer())
                    
                    # st.toast(f"The mesh file has been saved: {file_path}", icon="ğŸ’¾")
                    st.toast(f"ç½‘æ ¼æ–‡ä»¶å·²ä¿å­˜ï¼š{file_path}", icon="ğŸ’¾")

                    config.case_grid = f"{config.TEMP_PATH}/{uploaded_mesh_file.name}"

                    # check the grid using OpenFOAM, later
                    
                    case_file_requirements.extract_boundary_names(file_path)

                    # st.toast(f"The mesh file has been processed! ")
                    st.toast("ç½‘æ ¼æ–‡ä»¶å¤„ç†å®Œæˆï¼")

                    boundary_names = ", ".join(config.case_boundaries)

                    config.case_boundary_names = boundary_names

                    # info_after_mesh_processed = f'''You have uploaded a mesh file with boundary names as: {boundary_names}.\nNow the case are prepared and running in the background. Running information will be shown in the console.'''
                    info_after_mesh_processed = f'''ä½ ä¸Šä¼ çš„ç½‘æ ¼æ–‡ä»¶åŒ…å«ä»¥ä¸‹è¾¹ç•Œåç§°ï¼š{boundary_names}ã€‚\næ¡ˆä¾‹å·²å‡†å¤‡å®Œæ¯•å¹¶åœ¨åå°è¿è¡Œï¼Œè¿è¡Œä¿¡æ¯ä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°ã€‚'''
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": info_after_mesh_processed,
                        "timestamp": datetime.now().isoformat()
                    })

                    st.session_state.ask_case_solver = True

                    st.session_state.uploaded_grid = True

                except Exception as e:
                    # st.error(f"Failed at processed the mesh file: {str(e)}")              
                    st.error(f"å¤„ç†ç½‘æ ¼æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")              

    # Display conversation history
    if len(st.session_state.messages) > 0:
        for message in st.session_state.messages[1:]:
            if message["role"] == "user":
                st.chat_message("user").write(message["content"])
            else:
                if message["content"].startswith("Understand the user's answer") or message["content"].startswith("è¯·ç†è§£ç”¨æˆ·çš„å›ç­”"):
                    continue
                else:
                    st.chat_message("assistant").write(message["content"])

    if st.session_state.show_start == False:
        # st.header('**Please upload the paper to start!**')
        st.header('**è¯·ä¸Šä¼ è®ºæ–‡ä»¥å¼€å§‹ï¼**')
        st.session_state.show_start = True

    # guide the user to choose cases
    if st.session_state.ask_case_solver == True and st.session_state.user_answered == True:
        a = 1
        try: 
            user_answer = st.chat_messages[-1]['content']
            paper_case_descriptions = st.chat_messages[-1]['content']

            # json_reponse_sample = '''
            # {
            #     "Case_1":{
            #         "solver":"<solver_name>",
            #         "turbulence_model":"<model_name>",
            #         "other_physical_model":"<model_name>",
            #         "case_specific_description":"<specific case discription that differenciate this case from the others in the paper."
            #     },
            #     "Case_2":{
            #         "solver":"<solver_name>",
            #         "turbulence_model":"<model_name>",
            #         "other_physical_model":"<model_name>",
            #         "case_specific_description":"<specific case discription that differenciate this case from the others in the paper."
            #     },
            #     "Case_X":{
            #         "solver":"<solver_name>",
            #         "turbulence_model":"<model_name>",
            #         "other_physical_model":"<model_name>",
            #         "case_specific_description":"<specific case discription that differenciate this case from the others in the paper."
            #     }
            # }
            # '''
            json_reponse_sample = '''
            {
                "Case_1":{
                    "solver":"<æ±‚è§£å™¨åç§°>",
                    "turbulence_model":"<æ¹æµæ¨¡å‹åç§°>",
                    "other_physical_model":"<å…¶ä»–ç‰©ç†æ¨¡å‹åç§°>",
                    "case_specific_description":"<èƒ½å¤ŸåŒºåˆ†è¯¥æ¡ˆä¾‹ä¸è®ºæ–‡ä¸­å…¶ä»–æ¡ˆä¾‹çš„ç‰¹å¾æè¿°>"
                },
                "Case_2":{
                    "solver":"<æ±‚è§£å™¨åç§°>",
                    "turbulence_model":"<æ¹æµæ¨¡å‹åç§°>",
                    "other_physical_model":"<å…¶ä»–ç‰©ç†æ¨¡å‹åç§°>",
                    "case_specific_description":"<èƒ½å¤ŸåŒºåˆ†è¯¥æ¡ˆä¾‹ä¸è®ºæ–‡ä¸­å…¶ä»–æ¡ˆä¾‹çš„ç‰¹å¾æè¿°>"
                },
                "Case_X":{
                    "solver":"<æ±‚è§£å™¨åç§°>",
                    "turbulence_model":"<æ¹æµæ¨¡å‹åç§°>",
                    "other_physical_model":"<å…¶ä»–ç‰©ç†æ¨¡å‹åç§°>",
                    "case_specific_description":"<èƒ½å¤ŸåŒºåˆ†è¯¥æ¡ˆä¾‹ä¸è®ºæ–‡ä¸­å…¶ä»–æ¡ˆä¾‹çš„ç‰¹å¾æè¿°>"
                }
            }
            '''

            # guide_case_choose_prompt = f'''Understand the user's answer and describe the case details of the user's requirement.
            #
            #             The user's answer is:{user_answer}
            #
            #             Please generate JSON content according to these requirements:
            #
            #             1. Strictly follow this example format containing ONLY JSON content:{json_reponse_sample}. For the case_specific_description sections, propose characteristics that can differenciate this case from the other similar cases in the paper. The differentiating characteristics must exclude conventional attributes such as geometry, shape, numerical parameters, physical models, or other standard descriptors. 
            #
            #             2. Absolutely AVOID any non-JSON elements including but not limited to:
            #             - Markdown code block markers (```json or ```)
            #             - Extra comments or explanations
            #             - Unnecessary empty lines or indentation
            #             - Any text outside JSON structure
            #
            #             3. Critical syntax requirements:
            #             - Maintain strict JSON syntax compliance
            #             - Enclose all keys in double quotes
            #             - Use double quotes for string values
            #             - Ensure no trailing comma after last property
            # '''
            guide_case_choose_prompt = f'''è¯·ç†è§£ç”¨æˆ·çš„å›ç­”ï¼Œå¹¶æè¿°å…¶éœ€æ±‚å¯¹åº”çš„æ¡ˆä¾‹ç»†èŠ‚ã€‚

                        ç”¨æˆ·çš„å›ç­”æ˜¯:{user_answer}

                        è¯·æŒ‰ä»¥ä¸‹è¦æ±‚ç”Ÿæˆ JSON å†…å®¹ï¼š

                        1. ä¸¥æ ¼éµå¾ªä»…åŒ…å« JSON çš„ç¤ºä¾‹æ ¼å¼ï¼š{json_reponse_sample}ã€‚å¯¹äº case_specific_description å­—æ®µï¼Œè¯·æå‡ºèƒ½å°†è¯¥æ¡ˆä¾‹ä¸è®ºæ–‡ä¸­å…¶ä»–ç›¸ä¼¼æ¡ˆä¾‹åŒºåˆ†å¼€çš„ç‰¹å¾ï¼Œä¸”è¿™äº›ç‰¹å¾ä¸å¾—åŒ…å«å‡ ä½•ã€å½¢çŠ¶ã€æ•°å€¼å‚æ•°ã€ç‰©ç†æ¨¡å‹æˆ–å…¶ä»–å¸¸è§„æè¿°ã€‚

                        2. ä¸¥ç¦å‡ºç° JSON ä»¥å¤–çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
                        - Markdown ä»£ç å—æ ‡è®°ï¼ˆ```json æˆ– ```ï¼‰
                        - é¢å¤–æ³¨é‡Šæˆ–è§£é‡Š
                        - ä¸å¿…è¦çš„ç©ºè¡Œæˆ–ç¼©è¿›
                        - ä»»ä½• JSON ç»“æ„ä¹‹å¤–çš„æ–‡æœ¬

                        3. ä¸¥æ ¼éµå®ˆ JSON è¯­æ³•ï¼š
                        - æ‰€æœ‰é”®å¿…é¡»ä½¿ç”¨åŒå¼•å·
                        - å­—ç¬¦ä¸²å€¼å¿…é¡»ä½¿ç”¨åŒå¼•å·
                        - æœ€åä¸€ä¸ªå±æ€§åä¸å¾—å‡ºç°å¤šä½™é€—å·
            '''

            st.chat_message("assistant").write(guide_case_choose_prompt)
            st.session_state.messages.append({"role": "assistant", "content": guide_case_choose_prompt, "timestamp": datetime.now().isoformat()})

            with st.chat_message("assistant"):
                response = st.session_state.chatbot.get_response(st.session_state.messages)
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response, "timestamp": datetime.now().isoformat()})

            # prompt_2 = f'''Task: The user want to simulate a CFD case with the following characteristicis,
            # identify the CFD case from the following case descriptions from a PDF.
            # - Characteristics: {user_answer}.
            # - Case descriptions: {paper_case_descriptions}.
            # Your response shall only include the answer without any thinking content.
            # '''
            prompt_2 = f'''ä»»åŠ¡ï¼šç”¨æˆ·å¸Œæœ›æ¨¡æ‹Ÿå…·å¤‡ä»¥ä¸‹ç‰¹å¾çš„ CFD æ¡ˆä¾‹ï¼Œè¯·ä» PDF ä¸­çš„æ¡ˆä¾‹æè¿°é‡Œè¯†åˆ«è¯¥æ¡ˆä¾‹ã€‚
            - æ¡ˆä¾‹ç‰¹å¾ï¼š{user_answer}.
            - æ¡ˆä¾‹æè¿°ï¼š{paper_case_descriptions}.
            ä»…è¾“å‡ºç­”æ¡ˆï¼Œä¸å¾—åŒ…å«æ€è€ƒè¿‡ç¨‹ã€‚
            '''

        except Exception as e:
            # return f"Chat error: {str(e)}"
            return f"èŠå¤©å‡ºé”™ï¼š{str(e)}"

    # User input
    # if prompt := st.chat_input("Enter your requirement or reply."):
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚æˆ–å›å¤ã€‚"):
        
        st.chat_message("user").write(prompt)  # Display the user's original prompt in the UI

        if st.session_state.ask_case_solver and not st.session_state.user_answer_finished: # ask the user for Case_X, solver and turbulence
            # json_reponse_sample = '''
            # {
            #     "Case_1":{
            #         "case_name" = <some_case_name>,
            #         "solver":"<solver_name>",
            #         "turbulence_model":"<model_name>",
            #         "other_physical_model":"<model_name>",
            #         "case_specific_description":"<a sentence that describes the case setup with detailed parameters that differenciate this case from the other cases in the paper>"
            #     }
            # }
            # '''
            json_reponse_sample = '''
            {
                "Case_1":{
                    "case_name" = <æ¡ˆä¾‹åç§°>,
                    "solver":"<æ±‚è§£å™¨åç§°>",
                    "turbulence_model":"<æ¹æµæ¨¡å‹åç§°>",
                    "other_physical_model":"<å…¶ä»–ç‰©ç†æ¨¡å‹åç§°>",
                    "case_specific_description":"<ä¸€æ®µèƒ½å¤Ÿé€šè¿‡è¯¦ç»†å‚æ•°åŒºåˆ†è¯¥æ¡ˆä¾‹çš„æè¿°>"
                }
            }
            '''

            # guide_case_choose_prompt = f'''Understand the user's answer and describe the case details of the user's requirement.
            #
            #             The user's answer is:{prompt}
            #
            #             Please generate JSON content according to these requirements:
            #
            #             1. Strictly follow this example format containing ONLY JSON content:{json_reponse_sample}
            #
            #             2. Absolutely AVOID any non-JSON elements including but not limited to:
            #             - Markdown code block markers (```json or ```)
            #             - Extra comments or explanations
            #             - Unnecessary empty lines or indentation
            #             - Any text outside JSON structure
            #
            #             3. Critical syntax requirements:
            #             - Maintain strict JSON syntax compliance
            #             - Enclose all keys in double quotes
            #             - Use double quotes for string values
            #             - Ensure no trailing comma after last property
            #
            #             4. Case_name must adhere to the following format:
            #              [a-zA-Z0-9_]+ - only containing lowercase letters, uppercase letters, numbers, or underscores. Special characters (e.g. -, @, #, spaces) are not permitted.
            #
            #             5. The solver must be one of the followings: {config.string_of_solver_keywords}. 
            #             The turbulence _model must be one of the followings: {config.string_of_turbulence_model}.
            # '''
            guide_case_choose_prompt = f'''è¯·ç†è§£ç”¨æˆ·çš„å›ç­”ï¼Œå¹¶æè¿°å…¶éœ€æ±‚å¯¹åº”çš„æ¡ˆä¾‹ç»†èŠ‚ã€‚

                        ç”¨æˆ·çš„å›ç­”æ˜¯:{prompt}

                        è¯·æŒ‰ä»¥ä¸‹è¦æ±‚ç”Ÿæˆ JSON å†…å®¹ï¼š

                        1. ä¸¥æ ¼éµå¾ªä»…åŒ…å« JSON çš„ç¤ºä¾‹æ ¼å¼ï¼š{json_reponse_sample}

                        2. ä¸¥ç¦å‡ºç° JSON ä»¥å¤–çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
                        - Markdown ä»£ç å—æ ‡è®°ï¼ˆ```json æˆ– ```ï¼‰
                        - é¢å¤–æ³¨é‡Šæˆ–è§£é‡Š
                        - ä¸å¿…è¦çš„ç©ºè¡Œæˆ–ç¼©è¿›
                        - ä»»ä½• JSON ç»“æ„ä¹‹å¤–çš„æ–‡æœ¬

                        3. ä¸¥æ ¼éµå®ˆ JSON è¯­æ³•ï¼š
                        - æ‰€æœ‰é”®å¿…é¡»ä½¿ç”¨åŒå¼•å·
                        - å­—ç¬¦ä¸²å€¼å¿…é¡»ä½¿ç”¨åŒå¼•å·
                        - æœ€åä¸€ä¸ªå±æ€§åä¸å¾—å‡ºç°å¤šä½™é€—å·

                        4. case_name å¿…é¡»æ»¡è¶³æ ¼å¼ [a-zA-Z0-9_]+ï¼Œåªå…è®¸å­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿ï¼Œç¦æ­¢ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ -, @, #ã€ç©ºæ ¼ç­‰ï¼‰ã€‚

                        5. Solver å¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ï¼š{config.string_of_solver_keywords}ã€‚
                        æ¹æµæ¨¡å‹å¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ï¼š{config.string_of_turbulence_model}ã€‚
            '''

            st.session_state.messages.append({"role": "user", "content": guide_case_choose_prompt, "timestamp": datetime.now().isoformat()})

            # Get assistant's response
            with st.chat_message("assistant"):
                response = st.session_state.chatbot.get_response(st.session_state.messages)
                parsed_case_dict = _extract_json_dict(response)
                if not parsed_case_dict:
                    st.error("åŠ©æ‰‹è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œè¯·é‡è¯•æˆ–è°ƒæ•´æè¿°ã€‚")
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response,
                        "timestamp": datetime.now().isoformat()
                    })
                    return
                config.all_case_dict = parsed_case_dict

                qa = qa_modules.QA_NoContext_deepseek_R1()

                # convert_json_to_md = f'''Convert the provided JSON string into a Markdown format where:
                #     1. Each top-level JSON key becomes a main heading (#)
                #     2. Its corresponding key-value pairs are rendered as unordered list items
                #     3. Maintain the original key-value hierarchy in list format
                #
                #     The provided json string is as follow:{response}.
                # '''
                convert_json_to_md = f'''è¯·å°†ä»¥ä¸‹ JSON å­—ç¬¦ä¸²è½¬æ¢ä¸º Markdownï¼š
                    1. æ¯ä¸ªé¡¶å±‚ JSON é”®ä½œä¸ºä¸€çº§æ ‡é¢˜ï¼ˆ#ï¼‰
                    2. å…¶å¯¹åº”çš„é”®å€¼å¯¹ä»¥æ— åºåˆ—è¡¨å±•ç¤º
                    3. ä¿æŒåŸæœ‰çš„å±‚çº§ç»“æ„

                    éœ€è¦è½¬æ¢çš„ JSON å­—ç¬¦ä¸²å¦‚ä¸‹ï¼š{response}.
                '''

                md_form = qa.ask(convert_json_to_md)

                # decorated_response = f'''You choose to simulate the cases with the following setups:\n{md_form}'''
                decorated_response = f'''ä½ é€‰æ‹©æ¨¡æ‹Ÿçš„æ¡ˆä¾‹é…ç½®å¦‚ä¸‹ï¼š\n{md_form}'''
                st.write(decorated_response)
                st.session_state.messages.append({"role": "assistant", "content": decorated_response, "timestamp": datetime.now().isoformat()})
                # later, fnae
                st.session_state.user_answer_finished = True

                

        else:   # normal case
            st.session_state.messages.append({"role": "user", "content": prompt, "timestamp": datetime.now().isoformat()})
            # Get assistant's response
            with st.chat_message("assistant"):
                response = st.session_state.chatbot.get_response(st.session_state.messages)
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response, "timestamp": datetime.now().isoformat()})

    if st.session_state.file_processed and st.session_state.user_answer_finished and not st.session_state.uploaded_grid:
        # st.write("If you don't have further requirement on the case setup. \n**Please upload the mesh of the Fluent .msh format.**")
        st.write("å¦‚æœä½ å¯¹æ¡ˆä¾‹è®¾ç½®æ²¡æœ‰æ›´å¤šè¦æ±‚ã€‚\n**è¯·ä¸Šä¼  Fluent .msh æ ¼å¼çš„ç½‘æ ¼ã€‚**")

    if st.session_state.uploaded_grid and st.session_state.file_processed and st.session_state.user_answer_finished:
        # read in preprocess OF tutorials
        # print(f"**************** Preprocessing OF tutorials at {config.of_tutorial_dir} ****************")
        print(f"**************** åœ¨ {config.of_tutorial_dir} é¢„å¤„ç† OF æ•™ç¨‹ ****************")
        # if not config.flag_OF_tutorial_processed:
        #     preprocess_OF_tutorial.main()
        #     config.flag_OF_tutorial_processed = True
        preprocess_OF_tutorial.read_in_processed_merged_OF_cases()
        for key, value in config.all_case_dict.items():
            case_name = value["case_name"]
            # print(f"***** start processing {key}: {case_name} *****")
            print(f"***** å¼€å§‹å¤„ç† {key}: {case_name} *****")
            solver = value["solver"]
            turbulence_model = value["turbulence_model"]

            case_specific_description = value["case_specific_description"]

            main_run_chatcfd.test_solver = solver

            main_run_chatcfd.test_turbulence_model = turbulence_model

            main_run_chatcfd.test_case_name = case_name

            main_run_chatcfd.test_case_description = case_specific_description

            main_run_chatcfd.run_case()

            # single_case_builder_runner.single_case_details_from_PDF(case_name, solver, turbulence_model, 
            #     transient, simulation_duration, case_specific_description)

if __name__ == "__main__":
    set_config.read_in_config()
    # set_config.load_openfoam_environment()
    main()